{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0pbTBd0hZ0FsdGgdaXiH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbbonice2/PROJET-RL/blob/main/comparaison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsfrnkMvA-Ga"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RowGridWorld:\n",
        "    def __init__(self, n_cols=5, goal_states=[4], reward=1, step_penalty=-0.1):\n",
        "        self.n_cols = n_cols\n",
        "        self.goal_states = goal_states  # Liste des états cibles\n",
        "        self.reward = reward\n",
        "        self.step_penalty = step_penalty\n",
        "        self.state = 0\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(0, self.n_cols)  # Commencer à un état aléatoire\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, 0, True\n",
        "\n",
        "        if action == 0:  # Aller à gauche\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # Aller à droite\n",
        "            self.state = min(self.n_cols - 1, self.state + 1)\n",
        "\n",
        "        # Vérifier si l'agent atteint un bord de la grille\n",
        "        if self.state == 0 or self.state == self.n_cols - 1:\n",
        "            self.done = True  # Fin de l'épisode\n",
        "            reward = self.reward if self.state in self.goal_states else self.step_penalty\n",
        "            return self.state, reward, self.done  # Fin de l'épisode avec la récompense appropriée\n",
        "\n",
        "        # Calcul de la récompense pour chaque étape\n",
        "        if self.state in self.goal_states:  # Si l'état actuel est un état cible\n",
        "            reward = self.reward  # Récompense pour atteindre l'objectif\n",
        "        else:\n",
        "            reward = self.step_penalty  # Pénalité pour chaque mouvement effectué\n",
        "\n",
        "        return self.state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        grid = ['-' for _ in range(self.n_cols)]\n",
        "        grid[self.state] = 'A'\n",
        "        print(\" \".join(grid))\n",
        "\n",
        "# Algorithmes\n",
        "def greedy(env, n_episodes):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)  # Valeurs estimées pour chaque action (gauche, droite)\n",
        "    N = np.zeros(2)  # Nombre de fois que chaque action a été choisie\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.argmax(Q)  # Greedy: Choisir l'action avec la plus haute valeur estimée\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]  # Mise à jour de la valeur estimée\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def epsilon_greedy(env, n_episodes, epsilon=0.3):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)\n",
        "    N = np.zeros(2)\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice([0, 1])  # Exploration\n",
        "            else:\n",
        "                action = np.argmax(Q)  # Exploitation\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def ucb(env, n_episodes, c=2):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)\n",
        "    N = np.zeros(2)\n",
        "    t = 0\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            t += 1\n",
        "            action = np.argmax(Q + c * np.sqrt(np.log(t + 1) / (N + 1e-5)))  # UCB\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def gradient_bandit(env, n_episodes, alpha=0.3):\n",
        "    rewards = []\n",
        "    H = np.zeros(2)  # Valeurs de préférence pour chaque action\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            # Sélectionner une action selon les probabilités softmax\n",
        "            exp = np.exp(H)\n",
        "            probs = exp / np.sum(exp)\n",
        "            action = np.random.choice([0, 1], p=probs)\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Mise à jour des préférences H\n",
        "            baseline = np.mean(reward)\n",
        "            H[action] += alpha * (reward - baseline)\n",
        "\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "# Initialisation de l'environnement\n",
        "env = RowGridWorld(n_cols=5, goal_states=[2, 4], reward=1, step_penalty=-0.1)\n",
        "n_episodes = 100\n",
        "\n",
        "# Comparaison des méthodes\n",
        "greedy_rewards = greedy(env, n_episodes)\n",
        "epsilon_greedy_rewards = epsilon_greedy(env, n_episodes)\n",
        "ucb_rewards = ucb(env, n_episodes)\n",
        "gradient_bandit_rewards = gradient_bandit(env, n_episodes)\n",
        "\n",
        "# Tracer les courbes des récompenses moyennes cumulatives\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Moyennes cumulatives\n",
        "plt.plot(np.cumsum(greedy_rewards) / (np.arange(n_episodes) + 1), label='Greedy')\n",
        "plt.plot(np.cumsum(epsilon_greedy_rewards) / (np.arange(n_episodes) + 1), label='Epsilon-Greedy')\n",
        "plt.plot(np.cumsum(ucb_rewards) / (np.arange(n_episodes) + 1), label='UCB')\n",
        "plt.plot(np.cumsum(gradient_bandit_rewards) / (np.arange(n_episodes) + 1), label='Gradient Bandit')\n",
        "\n",
        "# Paramètres du graphique\n",
        "plt.xlabel('Épisodes')\n",
        "plt.ylabel('Récompense moyenne cumulative')\n",
        "plt.title('Comparaison entre Greedy, Epsilon-Greedy, UCB et Gradient Bandit')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ]
}