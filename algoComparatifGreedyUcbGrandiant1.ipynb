{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbbonice2/PROJET-RL/blob/main/algoComparatifGreedyUcbGrandiant1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avu5P5Y0KUJf"
      },
      "source": [
        "\n",
        "## **Comparaison Greedy , Epsilon-greedy , UCB et Gradient bandit**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsfrnkMvA-Ga"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RowGridWorld:\n",
        "    def __init__(self, n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1):\n",
        "        self.n_cols = n_cols\n",
        "        self.goal_states = goal_states  # États cibles avec récompense positive\n",
        "        self.neutral_states = neutral_states  # États neutres avec récompense 0\n",
        "        self.reward = reward\n",
        "        self.step_penalty = step_penalty\n",
        "        self.state = 0\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(0, self.n_cols)  # Commencer à un état aléatoire\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, 0, True\n",
        "\n",
        "        if action == 0:  # Aller à gauche\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # Aller à droite\n",
        "            self.state = min(self.n_cols - 1, self.state + 1)\n",
        "\n",
        "        # Vérifier si l'agent atteint un bord de la grille\n",
        "        if self.state == 0 or self.state == self.n_cols - 1:\n",
        "            self.done = True\n",
        "            reward = self.reward if self.state in self.goal_states else self.step_penalty\n",
        "            return self.state, reward, self.done\n",
        "\n",
        "        # Calcul de la récompense\n",
        "        if self.state in self.goal_states:  # Si l'état actuel est un état cible\n",
        "            reward = self.reward\n",
        "        elif self.state in self.neutral_states:  # Si l'état actuel est un état neutre\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = self.step_penalty  # Pénalité pour chaque mouvement\n",
        "\n",
        "        return self.state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        grid = ['-' for _ in range(self.n_cols)]\n",
        "        grid[self.state] = 'A'\n",
        "        print(\" \".join(grid))\n",
        "\n",
        "\n",
        "# Algorithmes\n",
        "def greedy(env, n_episodes):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)  # Valeurs estimées pour chaque action (gauche, droite)\n",
        "    N = np.zeros(2)  # Nombre de fois que chaque action a été choisie\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.argmax(Q)  # Greedy: Choisir l'action avec la plus haute valeur estimée\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]  # Mise à jour de la valeur estimée\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def epsilon_greedy(env, n_episodes, epsilon=0.3):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)\n",
        "    N = np.zeros(2)\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice([0, 1])  # Exploration\n",
        "            else:\n",
        "                action = np.argmax(Q)  # Exploitation\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def ucb(env, n_episodes, c=2):\n",
        "    rewards = []\n",
        "    Q = np.zeros(2)\n",
        "    N = np.zeros(2)\n",
        "    t = 0\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            t += 1\n",
        "            action = np.argmax(Q + c * np.sqrt(np.log(t + 1) / (N + 1e-5)))  # UCB\n",
        "            next_state, reward, done = env.step(action)\n",
        "            N[action] += 1\n",
        "            Q[action] += (reward - Q[action]) / N[action]\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "def gradient_bandit(env, n_episodes, alpha=0.3):\n",
        "    rewards = []\n",
        "    H = np.zeros(2)  # Valeurs de préférence pour chaque action\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            # Sélectionner une action selon les probabilités softmax\n",
        "            exp = np.exp(H)\n",
        "            probs = exp / np.sum(exp)\n",
        "            action = np.random.choice([0, 1], p=probs)\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Mise à jour des préférences H\n",
        "            baseline = np.mean(reward)\n",
        "            H[action] += alpha * (reward - baseline)\n",
        "\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    return np.array(rewards)\n",
        "\n",
        "# Initialisation de l'environnement\n",
        "env = RowGridWorld(n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1)\n",
        "n_episodes = 900\n",
        "\n",
        "# Comparaison des méthodes\n",
        "greedy_rewards = greedy(env, n_episodes)\n",
        "epsilon_greedy_rewards = epsilon_greedy(env, n_episodes)\n",
        "ucb_rewards = ucb(env, n_episodes)\n",
        "gradient_bandit_rewards = gradient_bandit(env, n_episodes)\n",
        "\n",
        "# Tracer les courbes des récompenses moyennes cumulatives\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Moyennes cumulatives\n",
        "plt.plot(np.cumsum(greedy_rewards) / (np.arange(n_episodes) + 1), label='Greedy')\n",
        "plt.plot(np.cumsum(epsilon_greedy_rewards) / (np.arange(n_episodes) + 1), label='Epsilon-Greedy')\n",
        "plt.plot(np.cumsum(ucb_rewards) / (np.arange(n_episodes) + 1), label='UCB')\n",
        "plt.plot(np.cumsum(gradient_bandit_rewards) / (np.arange(n_episodes) + 1), label='Gradient Bandit')\n",
        "\n",
        "# Paramètres du graphique\n",
        "plt.xlabel('Épisodes')\n",
        "plt.ylabel('Récompense moyenne cumulative')\n",
        "plt.title('Comparaison entre Greedy, Epsilon-Greedy, UCB et Gradient Bandit')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_-aEmM6Kqg1"
      },
      "source": [
        "## **comparaison entre Monte carlo prediction , Sarsa et N-step prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmH5mPkFKNdk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Classe RowGridWorld\n",
        "class RowGridWorld:\n",
        "    def __init__(self, n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1):\n",
        "        self.n_cols = n_cols\n",
        "        self.goal_states = goal_states  # États cibles avec récompense positive\n",
        "        self.neutral_states = neutral_states  # États neutres avec récompense 0\n",
        "        self.reward = reward\n",
        "        self.step_penalty = step_penalty\n",
        "        self.state = 0\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(0, self.n_cols)  # Commencer à un état aléatoire\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, 0, True\n",
        "\n",
        "        if action == 0:  # Aller à gauche\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # Aller à droite\n",
        "            self.state = min(self.n_cols - 1, self.state + 1)\n",
        "\n",
        "        # Vérifier si l'agent atteint un bord de la grille\n",
        "        if self.state == 0 or self.state == self.n_cols - 1:\n",
        "            self.done = True\n",
        "            reward = self.reward if self.state in self.goal_states else self.step_penalty\n",
        "            return self.state, reward, self.done\n",
        "\n",
        "        # Calcul de la récompense\n",
        "        if self.state in self.goal_states:  # Si l'état actuel est un état cible\n",
        "            reward = self.reward\n",
        "        elif self.state in self.neutral_states:  # Si l'état actuel est un état neutre\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = self.step_penalty  # Pénalité pour chaque mouvement\n",
        "\n",
        "        return self.state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        grid = ['-' for _ in range(self.n_cols)]\n",
        "        grid[self.state] = 'A'\n",
        "        print(\" \".join(grid))\n",
        "\n",
        "\n",
        "# Monte Carlo Prediction\n",
        "def monte_carlo_prediction(env, n_episodes, gamma=0.9):\n",
        "    V = np.zeros(env.n_cols)  # Valeurs des états\n",
        "    returns = {s: [] for s in range(env.n_cols)}  # Stocker les retours pour chaque état\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode = []\n",
        "\n",
        "        while not done:\n",
        "            action = np.random.choice([0, 1])  # Politique aléatoire\n",
        "            next_state, reward, done = env.step(action)\n",
        "            episode.append((state, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for state, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if state not in visited:\n",
        "                returns[state].append(G)\n",
        "                V[state] = np.mean(returns[state])\n",
        "                visited.add(state)\n",
        "\n",
        "        rewards_per_episode.append(sum(r for _, r in episode))\n",
        "    return V, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# SARSA\n",
        "def sarsa(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = np.zeros((env.n_cols, 2))  # Valeurs état-action\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
        "\n",
        "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "    return Q, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# n-Step Prediction\n",
        "def n_step_prediction(env, n_episodes, n=3, alpha=0.1, gamma=0.9):\n",
        "    V = np.zeros(env.n_cols)\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        states = [state]\n",
        "        rewards = []\n",
        "        T = float('inf')\n",
        "        total_reward = 0\n",
        "\n",
        "        t = 0\n",
        "        while True:\n",
        "            if t < T:\n",
        "                action = np.random.choice([0, 1])  # Politique aléatoire\n",
        "                next_state, reward, done = env.step(action)\n",
        "                rewards.append(reward)\n",
        "                states.append(next_state)\n",
        "                if done:\n",
        "                    T = t + 1\n",
        "\n",
        "            tau = t - n + 1\n",
        "            if tau >= 0:\n",
        "                G = sum(gamma ** (i - tau) * rewards[i] for i in range(tau, min(tau + n, T)))\n",
        "                if tau + n < T:\n",
        "                    G += gamma ** n * V[states[tau + n]]\n",
        "                V[states[tau]] += alpha * (G - V[states[tau]])\n",
        "\n",
        "            if tau == T - 1:\n",
        "                break\n",
        "            t += 1\n",
        "            total_reward += sum(rewards)\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "    return V, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# Initialisation de l'environnement\n",
        "env = RowGridWorld(n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1)\n",
        "n_episodes = 500\n",
        "\n",
        "# Comparaison des méthodes\n",
        "_, mc_rewards = monte_carlo_prediction(env, n_episodes)\n",
        "_, sarsa_rewards = sarsa(env, n_episodes)\n",
        "_, n_step_rewards = n_step_prediction(env, n_episodes)\n",
        "\n",
        "# Tracer les courbes des récompenses moyennes cumulatives\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(mc_rewards) / (np.arange(n_episodes) + 1), label='Monte Carlo Prediction')\n",
        "plt.plot(np.cumsum(sarsa_rewards) / (np.arange(n_episodes) + 1), label='SARSA')\n",
        "plt.plot(np.cumsum(n_step_rewards) / (np.arange(n_episodes) + 1), label='n-Step Prediction')\n",
        "\n",
        "# Paramètres du graphique\n",
        "plt.xlabel('Épisodes')\n",
        "plt.ylabel('Récompense moyenne cumulative')\n",
        "plt.title('Comparaison entre Monte Carlo Prediction, SARSA et n-Step Prediction')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pImnA5-pLZ25"
      },
      "source": [
        "## **comparaison entre Q-learning , Sarsa et N-step sarsa**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFYom1xcLqqr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Classe RowGridWorld\n",
        "class RowGridWorld:\n",
        "    def __init__(self, n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1):\n",
        "        self.n_cols = n_cols\n",
        "        self.goal_states = goal_states  # États cibles avec récompense positive\n",
        "        self.neutral_states = neutral_states  # États neutres avec récompense 0\n",
        "        self.reward = reward\n",
        "        self.step_penalty = step_penalty\n",
        "        self.state = 0\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(0, self.n_cols)  # Commencer à un état aléatoire\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.state, 0, True\n",
        "\n",
        "        if action == 0:  # Aller à gauche\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:  # Aller à droite\n",
        "            self.state = min(self.n_cols - 1, self.state + 1)\n",
        "\n",
        "        # Vérifier si l'agent atteint un bord de la grille\n",
        "        if self.state == 0 or self.state == self.n_cols - 1:\n",
        "            self.done = True\n",
        "            reward = self.reward if self.state in self.goal_states else self.step_penalty\n",
        "            return self.state, reward, self.done\n",
        "\n",
        "        # Calcul de la récompense\n",
        "        if self.state in self.goal_states:  # Si l'état actuel est un état cible\n",
        "            reward = self.reward\n",
        "        elif self.state in self.neutral_states:  # Si l'état actuel est un état neutre\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = self.step_penalty  # Pénalité pour chaque mouvement\n",
        "\n",
        "        return self.state, reward, self.done\n",
        "\n",
        "    def render(self):\n",
        "        grid = ['-' for _ in range(self.n_cols)]\n",
        "        grid[self.state] = 'A'\n",
        "        print(\" \".join(grid))\n",
        "\n",
        "\n",
        "# Q-Learning\n",
        "def q_learning(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = np.zeros((env.n_cols, 2))  # Valeurs état-action\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "    return Q, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# SARSA\n",
        "def sarsa(env, n_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = np.zeros((env.n_cols, 2))  # Valeurs état-action\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
        "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
        "            state, action = next_state, next_action\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "    return Q, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# n-Step SARSA\n",
        "def n_step_sarsa(env, n_episodes, n=3, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = np.zeros((env.n_cols, 2))  # Valeurs état-action\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
        "        states = [state]\n",
        "        actions = [action]\n",
        "        rewards = []\n",
        "        T = float('inf')\n",
        "        total_reward = 0\n",
        "\n",
        "        t = 0\n",
        "        while True:\n",
        "            if t < T:\n",
        "                next_state, reward, done = env.step(action)\n",
        "                rewards.append(reward)\n",
        "                if done:\n",
        "                    T = t + 1\n",
        "                else:\n",
        "                    next_action = np.random.choice([0, 1]) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
        "                    states.append(next_state)\n",
        "                    actions.append(next_action)\n",
        "\n",
        "            tau = t - n + 1\n",
        "            if tau >= 0:\n",
        "                G = sum(gamma ** (i - tau) * rewards[i] for i in range(tau, min(tau + n, T)))\n",
        "                if tau + n < T:\n",
        "                    G += gamma ** n * Q[states[tau + n], actions[tau + n]]\n",
        "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
        "\n",
        "            if tau == T - 1:\n",
        "                break\n",
        "            t += 1\n",
        "            total_reward += sum(rewards)\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "    return Q, np.array(rewards_per_episode)\n",
        "\n",
        "\n",
        "# Initialisation de l'environnement\n",
        "env = RowGridWorld(n_cols=10, goal_states=[1, 4, 9], neutral_states=[2, 3, 5], reward=1, step_penalty=-0.1)\n",
        "n_episodes = 500\n",
        "\n",
        "# Comparaison des méthodes\n",
        "_, q_learning_rewards = q_learning(env, n_episodes)\n",
        "_, sarsa_rewards = sarsa(env, n_episodes)\n",
        "_, n_step_sarsa_rewards = n_step_sarsa(env, n_episodes)\n",
        "\n",
        "# Tracer les courbes des récompenses moyennes cumulatives\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(q_learning_rewards) / (np.arange(n_episodes) + 1), label='Q-Learning')\n",
        "plt.plot(np.cumsum(sarsa_rewards) / (np.arange(n_episodes) + 1), label='SARSA')\n",
        "plt.plot(np.cumsum(n_step_sarsa_rewards) / (np.arange(n_episodes) + 1), label='n-Step SARSA')\n",
        "\n",
        "# Paramètres du graphique\n",
        "plt.xlabel('Épisodes')\n",
        "plt.ylabel('Récompense moyenne cumulative')\n",
        "plt.title('Comparaison entre Q-Learning, SARSA et n-Step SARSA')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcKTyLpkRts7TyZ6yU/DUi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}